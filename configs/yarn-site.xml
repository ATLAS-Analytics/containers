<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- This file is managed by puppet. Changes may be lost next puppet run. -->

<configuration>

<!-- this is temporary - containers running over physical memory, perhaps YARN-1747 helps one day. -->
<property>
<name>yarn.nodemanager.pmem-check-enabled</name>
<value>false</value>
</property>

    <property>
	<name>yarn.scheduler.minimum-allocation-mb</name>
	<value>512</value>
    </property>

    <property>
       <name>yarn.resourcemanager.admin.client.thread-count</name>
       <value>10</value>
    </property>

    <property>
	<name>yarn.scheduler.increment-allocation-mb</name>
	<value>512</value>
    </property>

    <property>
	<name>yarn.scheduler.maximum-allocation-mb</name>
	<value>8192</value>
    </property>

    <property>
	<name>yarn.nodemanager.resource.memory-mb</name>
	<value>505976</value>
    </property>

    <property>
	<name>yarn.nodemanager.resource.cpu-vcores</name>
	<value>44</value>
    </property>

    <property>
        <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
        <value>95.0</value>
    </property>

    <property>
	<name>yarn.resourcemanager.scheduler.class</name>
	<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
    </property>

    <property>
	<name>yarn.resourcemanager.nodes.include-path</name>
        <value>/etc/hadoop/conf/mapred.includes</value>
        <final>true</final>
    </property>

    <property>
	<name>yarn.resourcemanager.nodes.exclude-path</name>
        <value>/etc/hadoop/conf/excludes</value>
        <final>true</final>
    </property>

    <property>
	<name>yarn.resourcemanager.scheduler.monitor.enable</name>
	<value>true</value>
    </property>

    <property>
    <description>Classpath for typical applications.</description>
	<name>yarn.application.classpath</name>
	
	    <value>
		$HADOOP_CONF_DIR,
		$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
		$HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,
		$HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,
		$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
		/etc/hbase/conf,/usr/lib/hbase/*,/usr/lib/hbase/lib/*
	    </value>
	    </property>


    <property>
       <name>yarn.resourcemanager.rm.container-allocation.expiry-interval-ms</name>
       <value>7200000</value>
    </property>

    <property>
	<name>yarn.nodemanager.aux-services</name>
	<value>spark_shuffle,mapreduce_shuffle</value>
    </property>

    <property>
	<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
	<value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>


    <property>
	<name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
	<value>org.apache.spark.network.yarn.YarnShuffleService</value>
    </property>

    <property>
	<name>yarn.nodemanager.local-dirs</name>
	<value>/data1/yarn/local,/data10/yarn/local,/data11/yarn/local,/data12/yarn/local,/data13/yarn/local,/data14/yarn/local,/data15/yarn/local,/data16/yarn/local,/data17/yarn/local,/data18/yarn/local,/data19/yarn/local,/data2/yarn/local,/data20/yarn/local,/data21/yarn/local,/data22/yarn/local,/data23/yarn/local,/data24/yarn/local,/data25/yarn/local,/data26/yarn/local,/data27/yarn/local,/data28/yarn/local,/data29/yarn/local,/data3/yarn/local,/data30/yarn/local,/data31/yarn/local,/data32/yarn/local,/data33/yarn/local,/data34/yarn/local,/data35/yarn/local,/data36/yarn/local,/data37/yarn/local,/data38/yarn/local,/data39/yarn/local,/data4/yarn/local,/data40/yarn/local,/data41/yarn/local,/data42/yarn/local,/data43/yarn/local,/data44/yarn/local,/data45/yarn/local,/data46/yarn/local,/data47/yarn/local,/data48/yarn/local,/data5/yarn/local,/data6/yarn/local,/data7/yarn/local,/data8/yarn/local,/data9/yarn/local,/local1/yarn/local,/local2/yarn/local,/local3/yarn/local</value>
	<final>true</final>
    </property>

    <property>
	<name>yarn.nodemanager.log-dirs</name>
	<value>/data1/yarn/logs,/data10/yarn/logs,/data11/yarn/logs,/data12/yarn/logs,/data13/yarn/logs,/data14/yarn/logs,/data15/yarn/logs,/data16/yarn/logs,/data17/yarn/logs,/data18/yarn/logs,/data19/yarn/logs,/data2/yarn/logs,/data20/yarn/logs,/data21/yarn/logs,/data22/yarn/logs,/data23/yarn/logs,/data24/yarn/logs,/data25/yarn/logs,/data26/yarn/logs,/data27/yarn/logs,/data28/yarn/logs,/data29/yarn/logs,/data3/yarn/logs,/data30/yarn/logs,/data31/yarn/logs,/data32/yarn/logs,/data33/yarn/logs,/data34/yarn/logs,/data35/yarn/logs,/data36/yarn/logs,/data37/yarn/logs,/data38/yarn/logs,/data39/yarn/logs,/data4/yarn/logs,/data40/yarn/logs,/data41/yarn/logs,/data42/yarn/logs,/data43/yarn/logs,/data44/yarn/logs,/data45/yarn/logs,/data46/yarn/logs,/data47/yarn/logs,/data48/yarn/logs,/data5/yarn/logs,/data6/yarn/logs,/data7/yarn/logs,/data8/yarn/logs,/data9/yarn/logs,/local1/yarn/logs,/local2/yarn/logs,/local3/yarn/logs</value>
    </property>

    <property>
	<description>Where to aggregate logs</description>
	<name>yarn.nodemanager.remote-app-log-dir</name>
	<value>/var/log/hadoop-yarn/apps</value>
    </property>

    <property>
	<name>yarn.app.mapreduce.am.staging-dir</name>
	<value>/var/history</value>
    </property>

    <property>
	<name>yarn.log-aggregation-enable</name>
	<value>true</value>
    </property>

    <property>
	<name>yarn.log-aggregation.retain-seconds</name>
	<value>259200</value>
    </property>

    <property>
	<name>yarn.log-aggregation.retain-check-interval-seconds</name>
	<value>3600</value>
    </property>

    <property>
	<name>yarn.log.server.url</name>
	<value>ithdp1101.cern.ch:19888/jobhistory/logs</value>
    </property>

    <property>
	<name>yarn.nodemanager.log.retain-seconds</name>
	<value>85000</value>
    </property>

    <property>
	<name>yarn.nodemanager.recovery.enabled</name>
	<value>true</value>
    </property>

    <property>
	<name>yarn.nodemanager.address</name>
	<value>0.0.0.0:45454</value>	    <!-- needed for node manager restart/recovery -->
    </property>

    
	<!-- ResourceManager security configs -->
	
	
	<property>
	  <name>yarn.resourcemanager.principal</name>
	  <!--<value>yarn/ithdp1101.cern.ch@CERN.CH</value>-->
	  <value>yarn/_HOST@CERN.CH</value>
	</property>

	<!-- NodeManager security configs -->
	<property>
	  <name>yarn.nodemanager.keytab</name>
	  <value>/etc/hadoop/conf/yarn.keytab</value>	<!-- path to the YARN keytab -->
	</property>

	<property>
	  <name>yarn.nodemanager.principal</name>
	  <value>yarn/_HOST@CERN.CH</value>
	</property>

	<property>
	  <name>yarn.nodemanager.container-executor.class</name>
	  <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
	</property>

	<property>
	  <name>yarn.nodemanager.linux-container-executor.group</name>
	  <value>yarn</value>
	</property>

	<!-- end of Kerberos-specific configuration -->
    
    
	<property>
	  <name>yarn.scheduler.fair.preemption</name>
	  <value>true</value>
	</property>

	<property>
	  <name>yarn.scheduler.fair.update-interval-ms</name>
	  <value>2000</value>
	</property>

	<property>
	  <name>yarn.resourcemanager.scheduler.monitor.enable</name>
	  <value>true</value>
	</property>

	<property>
	  <name>yarn.resourcemanager.scheduler.monitor.policies</name>
	  <value>org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy</value>
	</property>

	<property>
	  <name>yarn.resourcemanager.monitor.capacity.preemption.monitoring_interval</name>
	  <value>10000</value>
	</property>

	<property>
	  <name>yarn.resourcemanager.monitor.capacity.preemption.monitoring_interval</name>
	  <value>10000</value><!-- default: 3000 -->
	</property>

    

    <!-- data locality -->
	<property>
	  <name>yarn.scheduler.fair.locality.threshold.node</name>
	  <value>0.7</value><!-- default: -1.0 -->
	</property>

	<property>
	  <name>yarn.scheduler.fair.locality.threshold.rack</name>
	  <value>0.8</value><!-- default: -1.0 -->
	</property>

	<property>
	  <name>yarn.scheduler.fair.locality-delay-node-ms</name>
	  <value>2500</value><!-- default: -1 -->
	</property>

	<property>
	  <name>yarn.scheduler.fair.locality-delay-rack-ms</name>
	  <value>5000</value><!-- default: -1 -->
	</property>
    <!-- end data locality -->

   <!-- High availability yarn -->

    <property>
	<name>yarn.resourcemanager.connect.retry-interval.ms</name>
	<value>2000</value>
    </property>

    <property>
	<name>yarn.resourcemanager.ha.enabled</name>
	<value>true</value>
    </property>

    <property>
	<name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
	<value>true</value>
    </property>

    <property>
	<name>yarn.resourcemanager.ha.automatic-failover.embedded</name>
	<value>true</value>
    </property>

    <property>
	<name>yarn.resourcemanager.cluster-id</name>
	<value>analytix</value>
    </property>

    <property>
	<name>yarn.resourcemanager.ha.rm-ids</name>
	<value>ithdp1101.cern.ch,itrac51111.cern.ch</value>
    </property>

    
    <property>
	<name>yarn.resourcemanager.recovery.enabled</name>
	<value>true</value>
    </property>

    <property>
	<name>yarn.resourcemanager.store.class</name>
	<value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>

    <property>
	<name>yarn.resourcemanager.zk-address</name>
	<value>ithdp1101.cern.ch:2181,itrac51111.cern.ch:2181,p05151113588139.cern.ch:2181</value>
    </property>

    <property>
        <name>yarn.resourcemanager.max-completed-applications</name>
        <value>10000</value>
    </property>

    <property>
	<name>yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms</name>
	<value>5000</value>
    </property>

    <property>
	<name>yarn.resourcemanager.work-preserving-recovery.enabled</name>
	<value>true</value>
    </property>

    <property>
	<name>yarn.resourcemanager.am.max-attempts</name>
	<value>8</value><!-- number of application master restarts - every resource-manager switch restarts all application masters -->
    </property>

    <!-- ResourceManager1 configs -->
    
    <property>
	<name>yarn.resourcemanager.hostname.ithdp1101.cern.ch</name>
	<value>ithdp1101.cern.ch</value>
    </property>

    <property>
        <name>yarn.resourcemanager.webapp.address.ithdp1101.cern.ch</name>
        <value>ithdp1101.cern.ch:8088</value>
    </property>

    <property>
        <name>yarn.resourcemanager.resource-tracker.address.ithdp1101.cern.ch</name>
        <value>ithdp1101.cern.ch:8031</value>
    </property>

    <property>
        <name>yarn.resourcemanager.address.ithdp1101.cern.ch</name>
        <value>ithdp1101.cern.ch:8032</value>
    </property>

    <property>
        <name>yarn.resourcemanager.scheduler.address.ithdp1101.cern.ch</name>
        <value>ithdp1101.cern.ch:8030</value>
    </property>

    <property>
        <name>yarn.resourcemanager.admin.address.ithdp1101.cern.ch</name>
        <value>ithdp1101.cern.ch:8033</value>
    </property>


    
    <property>
	<name>yarn.resourcemanager.hostname.itrac51111.cern.ch</name>
	<value>itrac51111.cern.ch</value>
    </property>

    <property>
        <name>yarn.resourcemanager.webapp.address.itrac51111.cern.ch</name>
        <value>itrac51111.cern.ch:8088</value>
    </property>

    <property>
        <name>yarn.resourcemanager.resource-tracker.address.itrac51111.cern.ch</name>
        <value>itrac51111.cern.ch:8031</value>
    </property>

    <property>
        <name>yarn.resourcemanager.address.itrac51111.cern.ch</name>
        <value>itrac51111.cern.ch:8032</value>
    </property>

    <property>
        <name>yarn.resourcemanager.scheduler.address.itrac51111.cern.ch</name>
        <value>itrac51111.cern.ch:8030</value>
    </property>

    <property>
        <name>yarn.resourcemanager.admin.address.itrac51111.cern.ch</name>
        <value>itrac51111.cern.ch:8033</value>
    </property>


    

  
</configuration>
